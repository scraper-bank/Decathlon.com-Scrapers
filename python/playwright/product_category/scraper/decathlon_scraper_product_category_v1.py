"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import asyncio
import json
import logging
import re
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List
from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    return f"decathlon_com_product_category_page_scraper_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    "server": "http://residential-proxy.scrapeops.io:8181",
    "username": "scrapeops",
    "password": API_KEY
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    appliedFilters: List[str] = field(default_factory=list)
    bannerImage: str = ""
    breadcrumbs: List[Dict[str, str]] = field(default_factory=list)
    categoryId: str = ""
    categoryName: str = ""
    categoryUrl: str = ""
    description: str = ""
    pagination: Dict[str, Any] = field(default_factory=dict)
    products: List[Dict[str, Any]] = field(default_factory=list)
    subcategories: List[Dict[str, Any]] = field(default_factory=list)
    # Self-healing fields
    name: Optional[str] = None
    price: Optional[float] = None
    features: List[str] = field(default_factory=list)
    images: List[Any] = field(default_factory=list)
    specifications: List[Any] = field(default_factory=list)
    reviews: List[Any] = field(default_factory=list)
    serialNumbers: List[Any] = field(default_factory=list)
    videos: List[Any] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData):
        item_key = hash(json.dumps(asdict(input_data), sort_keys=True))
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

async def extract_data(page: Page) -> Optional[ScrapedData]:
    """Extract structured data using Playwright, mirroring the Go logic."""
    try:
        # Helper: Strip HTML
        async def strip_html(html_str: str) -> str:
            clean = re.sub(r'<[^>]*>', ' ', html_str)
            return " ".join(clean.split())

        # Helper: Absolute URL
        def make_absolute_url(url_str: str) -> str:
            if not url_str: return ""
            if url_str.startswith(("http://", "https://")): return url_str
            if url_str.startswith("//"): return f"https:{url_str}"
            domain = "https://www.decathlon.com"
            if url_str.startswith("/"): return f"{domain}{url_str}"
            return f"{domain}/{url_str}"

        # Helper: Detect Currency
        def detect_currency(price_text: str) -> str:
            price_text = price_text.upper()
            currency_map = {
                "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
                "EUR": "EUR", "€": "EUR",
                "GBP": "GBP", "£": "GBP", "GB£": "GBP",
                "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
                "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
                "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
                "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
                "CHF": "CHF", "FR.": "CHF",
                "SEK": "SEK", "KR": "SEK",
                "NZD": "NZD", "NZ$": "NZD",
            }
            for code, currency in currency_map.items():
                if code in price_text: return currency
            return "USD"

        # Helper: Parse Price
        def parse_price(price_text: str) -> float:
            clean_text = price_text.replace(",", "")
            match = re.search(r'[\d,]+\.?\d*', clean_text)
            return float(match.group()) if match else 0.0

        data = ScrapedData()

        # JSON Data Extraction
        json_script = await page.locator("script#header-json").text_content()
        json_data = {}
        if json_script:
            try:
                json_data = json.loads(json_script)
            except: pass

        # Banner Image
        banner_meta = await page.locator("meta[property='og:image']").get_attribute("content")
        data.bannerImage = make_absolute_url(banner_meta) if banner_meta else ""

        # Breadcrumbs
        breadcrumb_items = await page.locator(".dkt-breadcrumbs-bar__item a").all()
        for item in breadcrumb_items:
            name = (await item.text_content() or "").strip()
            href = await item.get_attribute("href")
            if name:
                data.breadcrumbs.append({"name": name, "url": make_absolute_url(href or "")})

        # Category ID
        data.categoryId = str(json_data.get("id", ""))

        # Category Name Strategy
        category_name = ""
        # 1. JSON
        if json_data.get("title"):
            category_name = json_data["title"].strip()
        # 2. Meta
        if not category_name:
            og_title = await page.locator("meta[property='og:title']").get_attribute("content")
            if og_title: category_name = og_title.strip()
        # 3. Active Nav / H1 / Title Tag
        if not category_name:
            category_name = (await page.locator(".plp-menu-nav__button--active").first.text_content() or "").strip()
            if not category_name:
                category_name = (await page.locator("h1.collection-hero__title").first.text_content() or "").strip()
            if not category_name:
                title_tag = await page.title()
                category_name = title_tag.split("–")[0].split("|")[0].strip()

            # UI Cleanup
            category_name = " ".join(category_name.split())
            lower_name = category_name.lower()
            if "collection:" in lower_name:
                idx = lower_name.find("collection:")
                category_name = category_name[idx+len("collection:"):].strip()
            elif lower_name.startswith("collection "):
                category_name = category_name[11:].strip()

        data.categoryName = category_name.strip()

        # Category URL
        canonical = await page.locator("link[rel='canonical']").get_attribute("href")
        data.categoryUrl = make_absolute_url(canonical or "")

        # Description
        desc_meta = await page.locator("meta[name='description']").get_attribute("content")
        data.description = desc_meta or ""

        # Pagination
        count_text = await page.locator("#ProductCountDesktop").inner_text()
        total_results = 0
        count_match = re.search(r'(\d+)', count_text)
        if count_match:
            total_results = int(count_match.group(1))
        
        data.pagination = {
            "currentPage": 1,
            "nextPageUrl": None,
            "prevPageUrl": None,
            "resultsPerPage": None,
            "totalPages": 1,
            "totalResults": total_results,
        }

        # Products Extraction
        product_elements = await page.locator("li.grid__item").all()
        for s in product_elements:
            # Name extraction with deduplication logic
            name_el = s.locator("h3.card__heading a.full-unstyled-link").first
            p_name_raw = (await name_el.text_content() or "").strip()
            p_name_clean = " ".join(p_name_raw.split())
            
            words = p_name_clean.split()
            if words and len(words) % 2 == 0:
                half = len(words) // 2
                if words[:half] == words[half:]:
                    p_name_clean = " ".join(words[:half])
            
            if len(p_name_clean) > 0 and len(p_name_clean) % 2 == 0:
                mid = len(p_name_clean) // 2
                if p_name_clean[:mid] == p_name_clean[mid:]:
                    p_name_clean = p_name_clean[:mid].strip()

            if not p_name_clean: continue

            # Brand
            brand = ""
            brand_el = s.locator(".card_information .font-body-s").first
            # Note: Playwright logic handles "First" via .first. 
            # We also check for active sections if e-commerce logic suggests it.
            if await brand_el.count() > 0:
                raw_brand = (await brand_el.text_content() or "").strip()
                brand = raw_brand.split("\n")[0].strip()
                b_words = brand.split()
                if len(b_words) > 1 and b_words[0].lower() == b_words[1].lower():
                    brand = b_words[0]
                brand = brand.replace(p_name_clean, "").strip()

            # URL, Image, Price
            p_url = await s.locator("h3.card__heading a").first.get_attribute("href")
            p_image = await s.locator(".card__media img").first.get_attribute("src")
            p_price_text = await s.locator(".price-item--regular").first.inner_text() or ""
            
            # Product ID
            p_id = ""
            id_attr = await s.locator("h3.card__heading a").first.get_attribute("id")
            if id_attr: p_id = id_attr.split("-")[-1]

            # Rating
            rating_val = 0.0
            rating_text = await s.locator(".rating").first.get_attribute("aria-label") or ""
            rate_match = re.search(r'([\d.]+)', rating_text)
            if rate_match: rating_val = float(rate_match.group(1))

            # Review Count
            review_count = 0
            count_sel = s.locator(".rating-count span").first
            if await count_sel.count() == 0:
                count_sel = s.locator(".rating-count").first
            count_raw = await count_sel.inner_text() or ""
            c_match = re.search(r'[0-9,]+', count_raw)
            if c_match:
                review_count = int(c_match.group().replace(",", ""))

            data.products.append({
                "availability": "in_stock",
                "brand": brand,
                "currency": detect_currency(p_price_text),
                "image": make_absolute_url(p_image or ""),
                "isPrime": False,
                "isSponsored": False,
                "name": p_name_clean,
                "preDiscountPrice": None,
                "price": parse_price(p_price_text),
                "productId": p_id,
                "rating": rating_val,
                "reviewCount": review_count,
                "url": make_absolute_url(p_url or ""),
            })

        # Subcategories
        sub_elements = await page.locator(".plp-menu-nav__button").all()
        for sub in sub_elements:
            name = (await sub.text_content() or "").strip()
            if name:
                slug = name.lower().replace(" ", "-")
                data.subcategories.append({
                    "name": name,
                    "productCount": None,
                    "url": f"https://www.decathlon.com/collections/{slug}"
                })

        # Self-healing for No Products
        if not data.products:
            data.name = data.categoryName
            data.price = 0.0
            accordion_html = await page.locator(".accordion__content").inner_html() or ""
            data.description = await strip_html(accordion_html)

        return data
    except Exception as e:
        logger.error(f"Extraction error: {e}")
        return None

async def scrape_page(browser: Browser, url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    tries = 0
    success = False
    while tries <= retries and not success:
        context = None
        page = None
        try:
            context = await browser.new_context(
                ignore_https_errors=True,
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
            )
            page = await context.new_page()
            await stealth_async(page)
            
            async def block_resources(route, request):
                if request.resource_type in ["image", "media", "font"]:
                    await route.abort()
                else:
                    await route.continue_()
            
            await page.route("**/*", block_resources)
            await page.goto(url, wait_until="domcontentloaded", timeout=180000)
            await page.wait_for_timeout(2000)
            
            scraped_data = await extract_data(page)
            if scraped_data:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No data extracted from {url}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            if page: await page.close()
            if context: await context.close()
            tries += 1

async def concurrent_scraping(urls: List[str], max_concurrent: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy=PROXY_CONFIG,
            args=[
                "--no-sandbox", "--disable-setuid-sandbox", "--disable-dev-shm-usage",
                "--disable-accelerated-2d-canvas", "--no-first-run", "--no-zygote",
                "--disable-gpu", "--disable-web-security"
            ]
        )
        semaphore = asyncio.Semaphore(max_concurrent)
        async def limited_scrape(url):
            async with semaphore:
                await scrape_page(browser, url, pipeline, max_retries)
        
        tasks = [limited_scrape(url) for url in urls]
        await asyncio.gather(*tasks)
        await browser.close()

if __name__ == "__main__":
    urls = [
        "https://www.decathlon.com/collections/lifestyle-packs?collection=12110",
    ]
    logger.info("Starting concurrent scraping with Playwright + Stealth...")
    asyncio.run(concurrent_scraping(urls, max_concurrent=1, max_retries=3))
    logger.info("Scraping complete.")