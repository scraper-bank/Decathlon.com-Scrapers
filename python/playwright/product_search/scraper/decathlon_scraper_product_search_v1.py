"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import asyncio
import json
import logging
import re
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List
from datetime import datetime

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async

API_KEY = "YOUR-API_KEY"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    "server": "http://residential-proxy.scrapeops.io:8181",
    "username": "scrapeops",
    "password": API_KEY
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"decathlon_com_product_search_page_scraper_data_{timestamp}.jsonl"

@dataclass
class ScrapedData:
    breadcrumbs: List[Dict[str, str]] = field(default_factory=list)
    pagination: Dict[str, Any] = field(default_factory=dict)
    products: List[Dict[str, Any]] = field(default_factory=list)
    searchMetadata: Dict[str, Any] = field(default_factory=dict)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData):
        # Create a stable key from the product IDs to avoid duplicates in the file
        item_key = ",".join(sorted([str(p.get("productId")) for p in input_data.products]))
        if not item_key:
            item_key = str(input_data.searchMetadata.get("query"))
            
        if item_key in self.items_seen:
            logger.warning(f"Duplicate dataset found. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved results to {self.jsonl_filename}")

def make_absolute_url(url_str: str) -> str:
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return f"https:{url_str}"
    domain = "https://www.decathlon.com"
    if url_str.startswith("/"):
        return f"{domain}{url_str}"
    return f"{domain}/{url_str}"

def detect_currency(price_text: str) -> str:
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def parse_price(price_text: str) -> float:
    if not price_text:
        return 0.0
    clean_text = price_text.replace(",", "")
    match = re.search(r'[\d,]+\.?\d*', clean_text)
    if match:
        try:
            return float(match.group())
        except ValueError:
            return 0.0
    return 0.0

async def extract_data(page: Page) -> Optional[ScrapedData]:
    """Extract structured data using Playwright Locators to mimic Go logic."""
    try:
        # Breadcrumbs
        breadcrumbs = []
        breadcrumb_items = page.locator(".dkt-breadcrumbs-bar__item a")
        count = await breadcrumb_items.count()
        for i in range(count):
            item = breadcrumb_items.nth(i)
            name = (await item.inner_text()).strip()
            href = await item.get_attribute("href") or ""
            if name:
                breadcrumbs.append({
                    "name": name,
                    "url": make_absolute_url(href)
                })

        # Facets for Category Logic
        available_categories = []
        facet_labels = page.locator(".facets__item .facet-checkbox__text-label")
        facet_count = await facet_labels.count()
        for i in range(facet_count):
            label = (await facet_labels.nth(i).inner_text()).strip()
            if label:
                available_categories.append(label)

        # Products
        products = []
        product_cards = page.locator("li.grid__item")
        product_count = await product_cards.count()

        for i in range(product_count):
            card = product_cards.nth(i)
            
            # Name and URL
            name_el = card.locator(".card__heading a").first
            name = (await name_el.inner_text()).strip()
            if not name:
                name = (await card.locator("h3").first.inner_text()).strip()
            
            url_attr = await name_el.get_attribute("href") or ""
            product_url = make_absolute_url(url_attr)

            # Prices
            sale_price_el = card.locator(".price-item--sale").first
            reg_price_el = card.locator(".price-item--regular").first
            price_fallback_el = card.locator(".price").first

            price_text = ""
            if await sale_price_el.count() > 0:
                price_text = await sale_price_el.inner_text()
            elif await reg_price_el.count() > 0:
                price_text = await reg_price_el.inner_text()
            else:
                price_text = await price_fallback_el.inner_text()
            
            current_price = parse_price(price_text)
            
            pre_discount_price = None
            if await sale_price_el.count() > 0:
                old_price_text = await reg_price_el.inner_text()
                if old_price_text:
                    pre_discount_price = parse_price(old_price_text)

            # Brand
            brand = (await card.locator(".font-body-s").first.inner_text()).strip()
            if not brand:
                brand = "Quechua"

            # Ratings
            rating_val = 0.0
            review_count = 0
            rating_text = await card.locator(".rating-text span").first.inner_text()
            if rating_text:
                try:
                    rating_val = float(rating_text.strip())
                except ValueError:
                    pass
            
            count_text = await card.locator(".rating-count span").first.inner_text()
            if count_text:
                match = re.search(r'\d+[\d,]*', count_text.replace(",", ""))
                if match:
                    review_count = int(match.group())

            # Images
            img_el = card.locator(".card__media img").first
            img_src = await img_el.get_attribute("src") or ""
            img_alt = await img_el.get_attribute("alt") or ""

            # Product ID Logic
            p_id = ""
            card_link_id = await name_el.get_attribute("id") or ""
            if card_link_id and "--" in card_link_id:
                p_id = card_link_id.split("--")[1]
            
            if not p_id:
                badge_id = await card.locator(".badge").first.get_attribute("id") or ""
                if "--" in badge_id:
                    p_id = badge_id.split("--")[1]
            
            if not p_id:
                id_parts = url_attr.split("-")
                if id_parts:
                    last_part = id_parts[-1]
                    id_match = re.search(r'^\d+', last_part)
                    if id_match:
                        p_id = id_match.group()

            # Badges and Promotions
            badges = []
            promotions = []
            badge_el = card.locator(".badge").first
            if await badge_el.count() > 0:
                badge_text = (await badge_el.inner_text()).strip()
                if badge_text:
                    badges.append({
                        "imageUrl": None,
                        "label": badge_text,
                        "type": "deal"
                    })
                    promo_match = re.search(r'(\d+)%', badge_text)
                    if promo_match:
                        promotions.append({
                            "description": badge_text,
                            "discountPercentage": int(promo_match.group(1)),
                            "endDate": None,
                            "type": "discount"
                        })

            # Variants
            variants = []
            swatches = card.locator(".swatch")
            swatch_count = await swatches.count()
            for j in range(swatch_count):
                sw = swatches.nth(j)
                v_img = await sw.get_attribute("data-variant-image-src") or ""
                v_title = await sw.get_attribute("data-variant-title") or ""
                variants.append({
                    "imageUrl": make_absolute_url(v_img),
                    "type": "swatch",
                    "value": v_title
                })

            # Category Logic
            category = "Shoes"
            product_name_lower = name.lower()
            product_url_lower = product_url.lower()
            
            if "snow boots" in product_name_lower or "snow-boots" in product_url_lower:
                category = "Snow boots"
            elif "socks" in product_name_lower or "socks" in product_url_lower:
                category = "Socks"
            else:
                for cat in available_categories:
                    if cat.lower() in product_name_lower:
                        category = cat
                        break

            product_data = {
                "aggregateRating": {
                    "bestRating": 5,
                    "ratingValue": rating_val,
                    "reviewCount": review_count,
                    "worstRating": 1,
                },
                "availability": "in_stock",
                "availabilityMessage": None,
                "badges": badges,
                "brand": brand,
                "category": category,
                "currency": detect_currency(price_text),
                "description": None,
                "images": [{"altText": img_alt, "url": make_absolute_url(img_src)}],
                "isSponsored": False,
                "keyFeatures": [],
                "name": name,
                "preDiscountPrice": pre_discount_price,
                "price": current_price,
                "priceRange": {
                    "currency": detect_currency(price_text),
                    "maxPrice": pre_discount_price if pre_discount_price else current_price,
                    "minPrice": current_price,
                },
                "productId": p_id,
                "promotions": promotions,
                "seller": None,
                "shipping": None,
                "url": product_url,
                "variants": {
                    "variantCount": None,
                    "visibleOptions": variants,
                },
                "additionalImages": []
            }
            products.append(product_data)

        # Metadata
        total_results = 0
        count_text_el = page.locator("#ProductCountDesktop")
        if await count_text_el.count() > 0:
            count_str = await count_text_el.inner_text()
            match = re.search(r'\d+', count_str)
            if match:
                total_results = int(match.group())

        return ScrapedData(
            breadcrumbs=breadcrumbs,
            pagination={
                "currentPage": 1,
                "hasNextPage": False,
                "hasPreviousPage": False,
                "nextPageUrl": None,
                "previousPageUrl": None,
                "totalPages": 1,
            },
            products=products,
            searchMetadata={
                "query": "kids shoes",
                "resultsDisplayed": len(products),
                "searchType": "keyword",
                "searchUrl": page.url,
                "totalResults": total_results,
            }
        )
    except Exception as e:
        logger.error(f"Error extracting data: {e}")
        return None

async def scrape_page(browser: Browser, url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic and performance optimizations."""
    tries = 0
    success = False

    while tries <= retries and not success:
        context = None
        page = None
        try:
            context = await browser.new_context(
                ignore_https_errors=True,
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
            )
            
            page = await context.new_page()
            await stealth_async(page)
            
            async def block_resources(route, request):
                if request.resource_type in ["image", "media", "font"]:
                    await route.abort()
                else:
                    await route.continue_()
            
            await page.route("**/*", block_resources)
            
            await page.goto(url, wait_until="domcontentloaded", timeout=180000)
            
            # Use auto-waiting for the main product grid
            await page.wait_for_selector("li.grid__item", timeout=30000)
            
            scraped_data = await extract_data(page)
            
            if scraped_data and scraped_data.products:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No product data extracted from {url}")
        except Exception as e:
            logger.error(f"Try {tries + 1} - Exception scraping {url}: {e}")
        finally:
            if page: await page.close()
            if context: await context.close()
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

async def concurrent_scraping(urls: List[str], max_concurrent: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently with optimizations."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy=PROXY_CONFIG,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-accelerated-2d-canvas",
                "--no-first-run",
                "--no-zygote",
                "--disable-gpu",
                "--disable-web-security",
                "--disable-features=IsolateOrigins,site-per-process"
            ]
        )
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_scrape(url):
            async with semaphore:
                await scrape_page(browser, url, pipeline, max_retries)
        
        tasks = [limited_scrape(url) for url in urls]
        await asyncio.gather(*tasks)
        
        await browser.close()

if __name__ == "__main__":
    urls = [
        "https://www.decathlon.com/search?q=kids+shoes&options%5Bprefix%5D=last",
    ]

    logger.info("Starting concurrent scraping with Playwright + Stealth...")
    asyncio.run(concurrent_scraping(urls, max_concurrent=1, max_retries=3))
    logger.info("Scraping complete.")