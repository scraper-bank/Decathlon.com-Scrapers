"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import asyncio
import json
import logging
import re
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List, Union
from datetime import datetime

from playwright.async_api import async_playwright, Page, Browser
from playwright_stealth import stealth_async

# Standard configuration
API_KEY = "YOUR-API_KEY"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    "server": "http://residential-proxy.scrapeops.io:8181",
    "username": "scrapeops",
    "password": API_KEY
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"decathlon_com_product_page_scraper_data_{timestamp}.jsonl"

@dataclass
class ScrapedData:
    aggregateRating: Dict[str, Any] = field(default_factory=dict)
    availability: str = ""
    brand: str = ""
    category: str = ""
    currency: str = "USD"
    description: str = ""
    features: List[str] = field(default_factory=list)
    images: List[Dict[str, str]] = field(default_factory=list)
    name: str = ""
    preDiscountPrice: float = 0.0
    price: float = 0.0
    productId: str = ""
    reviews: List[Dict[str, Any]] = field(default_factory=list)
    seller: Dict[str, Any] = field(default_factory=dict)
    serialNumbers: List[Dict[str, str]] = field(default_factory=list)
    specifications: List[Dict[str, str]] = field(default_factory=list)
    url: str = ""
    videos: List[Dict[str, str]] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData) -> bool:
        item_key = input_data.productId or input_data.url
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

async def extract_data(page: Page) -> Optional[ScrapedData]:
    """Extract structured data using Playwright, replicating the Go scraper logic."""
    try:
        # Helper functions
        def make_absolute_url(url_str: str) -> str:
            if not url_str:
                return ""
            if url_str.startswith(("http://", "https://")):
                return url_str
            if url_str.startswith("//"):
                return "https:" + url_str
            domain = "https://www.decathlon.com"
            if url_str.startswith("/"):
                return f"{domain}{url_str}"
            return f"{domain}/{url_str}"

        def detect_currency(price_text: str) -> str:
            price_text = price_text.upper()
            currency_map = {
                "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
                "EUR": "EUR", "€": "EUR",
                "GBP": "GBP", "£": "GBP", "GB£": "GBP",
                "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
                "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
                "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
                "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
                "CHF": "CHF", "FR.": "CHF",
                "SEK": "SEK", "KR": "SEK",
                "NZD": "NZD", "NZ$": "NZD",
            }
            for code, currency in currency_map.items():
                if code in price_text:
                    return currency
            return "USD"

        def clean_numeric(s: str) -> float:
            if not s:
                return 0.0
            match = re.search(r'[\d,]+\.?\d*', s.replace(",", ""))
            if match:
                try:
                    return float(match.group())
                except ValueError:
                    return 0.0
            return 0.0

        def clean_img_url(u: str) -> str:
            if not u:
                return ""
            u = make_absolute_url(u)
            u = re.sub(r'([?&])(width|height|crop)=\d+', r'\1', u)
            u = u.rstrip('?').rstrip('&')
            u = u.replace("?&", "?").replace("&&", "&")
            return u

        # Extract JS variables and LD+JSON
        content = await page.content()
        
        json_ld_data = {}
        scripts = await page.locator("script[type='application/ld+json']").all_text_contents()
        for script_text in scripts:
            try:
                data = json.loads(script_text)
                if isinstance(data, dict) and data.get("@type") in ["Product", "ProductGroup"]:
                    json_ld_data = data
                    break
            except Exception:
                continue

        variant_data = {}
        variant_script = await page.locator("script[data-selected-variant]").first.text_content()
        if variant_script:
            try:
                variant_data = json.loads(variant_script)
            except Exception:
                pass

        # Aggregate Rating
        rating_val = clean_numeric(await page.locator("#averageRating").first.text_content() or "")
        if rating_val == 0:
            rating_val = clean_numeric(await page.locator(".rating-text span").first.text_content() or "")
        
        review_count_str = await page.locator(".rating-count span").first.text_content() or ""
        review_count = int(clean_numeric(review_count_str))

        aggregate_rating = {
            "bestRating": 5,
            "ratingValue": rating_val,
            "reviewCount": review_count,
            "worstRating": 1,
        }

        # Availability
        avail = "in_stock"
        if variant_data and not variant_data.get("available", True):
            avail = "out_of_stock"

        # Brand
        brand = ""
        if json_ld_data:
            brand = json_ld_data.get("brand", {}).get("name") if isinstance(json_ld_data.get("brand"), dict) else ""
        if not brand:
            brand = (await page.locator(".product__text").first.text_content() or "").strip()

        # Category
        category = ""
        if json_ld_data:
            category = json_ld_data.get("category", "")
        if not category:
            category = (await page.locator(".dkt-breadcrumbs-bar__item").last.text_content() or "").strip()

        # Prices
        price_text = await page.locator(".price-item--sale").first.text_content() or ""
        currency = detect_currency(price_text)
        current_price = clean_numeric(price_text)
        
        if current_price == 0 and variant_data:
            current_price = variant_data.get("price", 0) / 100

        pre_discount = 0.0
        # 1. Variant Data
        if variant_data:
            json_price = variant_data.get("price", 0) / 100
            json_compare = variant_data.get("compare_at_price", 0) / 100
            if json_compare > json_price and json_compare > 0:
                pre_discount = json_compare
                current_price = json_price

        # 2. CSS Scoped
        if pre_discount == 0:
            product_info = page.locator(".product__info-container, #MainContent, [id*='ProductInfo']").first
            scoped_sale_text = await product_info.locator(".price-item--sale").first.text_content() or ""
            scoped_sale_price = clean_numeric(scoped_sale_text)
            
            reg_price_text = await product_info.locator(".price--on-sale .price-item--regular, .price__sale .price-item--regular").first.text_content() or ""
            if not reg_price_text:
                reg_price_text = await product_info.locator(".old-price, .compare-at-price").first.text_content() or ""
            
            potential_pre = clean_numeric(reg_price_text)
            if potential_pre > scoped_sale_price and potential_pre > 0:
                pre_discount = potential_pre
                if current_price == 0:
                    current_price = scoped_sale_price

        # 3. JSON-LD
        if pre_discount == 0 and json_ld_data:
            offers = json_ld_data.get("offers", {})
            hp = 0.0
            if isinstance(offers, dict):
                hp = offers.get("highPrice", 0)
                if hp == 0 and isinstance(offers.get("offers"), list) and len(offers["offers"]) > 0:
                    hp = offers["offers"][0].get("highPrice", 0)
            if hp > 0 and (current_price == 0 or hp > current_price):
                pre_discount = float(hp)

        # Description
        desc = (await page.locator(".product__description").first.text_content() or "").strip()
        if not desc and json_ld_data:
            desc = json_ld_data.get("description", "")

        # Features
        features = []
        feature_items = await page.locator(".pdp-modal-features__item .pdp-modal-features__text").all_text_contents()
        features = [f.strip() for f in feature_items if f.strip()]

        # Images
        imgs = []
        seen_keys = set()

        # Strategy 1: Gallery
        gallery_items = await page.locator("media-gallery .product__media-list .product__media-item img").all()
        for img_el in gallery_items:
            if len(imgs) >= 10:
                break
            
            img_url = await img_el.get_attribute("src") or ""
            srcset = await img_el.get_attribute("srcset") or ""
            if srcset:
                parts = srcset.split(",")
                if parts:
                    last_part = parts[-1].strip()
                    fields = last_part.split()
                    if fields:
                        img_url = fields[0]
            
            if img_url:
                final_url = clean_img_url(img_url)
                alt = (await img_el.get_attribute("alt") or "").strip()
                
                is_noise = any(x in final_url for x in ["default_logo", "_default.png", "instafeed", "contents.mediadecathlon.com", ".svg"])
                seen_key = final_url.split("?")[0]

                if seen_key not in seen_keys and not is_noise:
                    seen_keys.add(seen_key)
                    imgs.append({"url": final_url, "alt_text": alt})

        # Fallback Strategy 2: Variant Data
        if not imgs and variant_data.get("images"):
            for u in variant_data["images"]:
                if len(imgs) >= 10:
                    break
                final_url = clean_img_url(u)
                seen_key = final_url.split("?")[0]
                if final_url and seen_key not in seen_keys:
                    seen_keys.add(seen_key)
                    imgs.append({"url": final_url, "alt_text": ""})

        # Fallback Strategy 3: JSON-LD
        if not imgs and json_ld_data.get("hasVariant"):
            for v in json_ld_data["hasVariant"]:
                if len(imgs) >= 10:
                    break
                u = v.get("image")
                if u:
                    final_url = clean_img_url(u)
                    seen_key = final_url.split("?")[0]
                    if final_url and seen_key not in seen_keys:
                        seen_keys.add(seen_key)
                        imgs.append({"url": final_url, "alt_text": ""})

        # Name
        name = (await page.locator("h1").first.text_content() or "").strip()
        if not name and json_ld_data:
            name = json_ld_data.get("name", "")

        # Product ID
        prod_id = ""
        if json_ld_data:
            prod_id = json_ld_data.get("productGroupID", "")
        if not prod_id:
            prod_id = await page.locator("product-info").get_attribute("data-product-id") or ""

        # Reviews
        reviews = []
        review_items = await page.locator(".pdp-reviews__item").all()
        for item in review_items:
            author_text = await item.locator(".pdp-reviews__author").text_content() or ""
            author = author_text.split("|")[0].strip()
            
            rating = 0.0
            # 1. Data attributes
            rating_attr = await item.locator("[data-rating]").first.get_attribute("data-rating")
            if rating_attr:
                rating = clean_numeric(rating_attr)
            else:
                # 2. Aria labels
                aria = await item.locator("[aria-label*='star' i], [aria-label*='rating' i]").first.get_attribute("aria-label")
                if aria:
                    rating = clean_numeric(aria)
                else:
                    # 4. Classes
                    cls = await item.locator("[class*='rating--'], [class*='stars--'], [class*='score-']").first.get_attribute("class")
                    if cls:
                        match = re.search(r'(?:rating|stars|score)[-_]+(\d+)', cls)
                        if match:
                            rating = float(match.group(1))

            if rating == 0:
                title = (await item.locator(".pdp-reviews__review-title").text_content() or "").lower()
                content_txt = (await item.locator(".pdp-reviews__review-text").text_content() or "").lower()
                full = f"{title} {content_txt}"
                
                neg = ["disappointed", "poor", "terrible", "awful", "waste", "bad", "return", "worst", "aliexpress", "temu"]
                pos = ["great", "excellent", "perfect", "love", "comfortable", "good", "highly recommend"]
                
                if any(s in full for s in neg):
                    rating = 1.0
                elif any(s in full for s in pos):
                    rating = 5.0
                else:
                    rating = 5.0

            reviews.append({
                "author": author,
                "content": (await item.locator(".pdp-reviews__review-text").text_content() or "").strip(),
                "date": (await item.locator(".pdp-reviews__review-meta span").last.text_content() or "").strip(),
                "rating": int(rating),
                "title": (await item.locator(".pdp-reviews__review-title").text_content() or "").strip(),
            })

        # Seller
        header_text = await page.locator("#header-json").text_content() or ""
        seller_name = "Decathlon"
        try:
            header_data = json.loads(header_text)
            seller_name = header_data.get("name", "Decathlon")
        except Exception:
            pass
        
        seller = {
            "name": seller_name,
            "rating": None,
            "url": "https://www.decathlon.com",
        }

        # Serials
        serials = []
        if variant_data:
            if variant_data.get("sku"):
                serials.append({"type": "SKU", "value": str(variant_data["sku"])})
            if variant_data.get("barcode"):
                serials.append({"type": "GTIN", "value": str(variant_data["barcode"])})
        
        model_code = await page.locator("#pdpModelCode").text_content() or ""
        model_code = model_code.replace("ID ", "").strip()
        if model_code:
            serials.append({"type": "Other", "value": model_code})

        # Specifications
        specs = []
        feature_containers = await page.locator(".pdp-modal-features__item").all()
        for container in feature_containers:
            key = (await container.locator(".pdp-modal-features__title").text_content() or "").strip()
            val = (await container.locator(".pdp-modal-features__text").text_content() or "").strip()
            if key and val:
                specs.append({"key": key, "value": val})
        
        coord_items = await page.locator(".composition-item").all()
        for item in coord_items:
            key_el = item.locator(".font-overline")
            key = (await key_el.text_content() or "").strip()
            full_text = await item.text_content() or ""
            val = full_text.replace(key, "").strip()
            key = key.rstrip(":")
            if key:
                specs.append({"key": key, "value": val})

        # URL
        canonical = await page.locator("link[rel='canonical']").get_attribute("href")
        final_url = make_absolute_url(canonical) if canonical else page.url

        # Videos
        videos = []
        video_tags = await page.locator("video").all()
        for v in video_tags:
            v_src = await v.get_attribute("src")
            if v_src:
                videos.append({"url": make_absolute_url(v_src)})

        return ScrapedData(
            aggregateRating=aggregate_rating,
            availability=avail,
            brand=brand,
            category=category,
            currency=currency,
            description=desc,
            features=features,
            images=imgs,
            name=name.strip(),
            preDiscountPrice=pre_discount,
            price=current_price,
            productId=prod_id,
            reviews=reviews,
            seller=seller,
            serialNumbers=serials,
            specifications=specs,
            url=final_url,
            videos=videos
        )

    except Exception as e:
        logger.error(f"Error extracting data: {e}")
        return None

async def scrape_page(browser: Browser, url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic and performance optimizations."""
    tries = 0
    success = False

    while tries <= retries and not success:
        context = None
        page = None
        try:
            context = await browser.new_context(
                ignore_https_errors=True,
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
            )
            
            page = await context.new_page()
            await stealth_async(page)
            
            async def block_resources(route, request):
                resource_type = request.resource_type
                if resource_type in ["image", "media", "font"]:
                    await route.abort()
                else:
                    await route.continue_()
            
            await page.route("**/*", block_resources)
            
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)
            await page.wait_for_timeout(2000) # Ensure scripts run
            
            scraped_data = await extract_data(page)
            
            if scraped_data:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No data extracted from {url}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            if page:
                await page.close()
            if context:
                await context.close()
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

async def concurrent_scraping(urls: List[str], max_concurrent: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently with optimizations."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy=PROXY_CONFIG,
            args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-accelerated-2d-canvas",
                "--no-first-run",
                "--no-zygote",
                "--disable-gpu",
                "--disable-web-security",
                "--disable-features=IsolateOrigins,site-per-process"
            ]
        )
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def limited_scrape(url):
            async with semaphore:
                await scrape_page(browser, url, pipeline, max_retries)
        
        tasks = [limited_scrape(url) for url in urls]
        await asyncio.gather(*tasks)
        
        await browser.close()

if __name__ == "__main__":
    urls = [
        "https://www.decathlon.com/products/quechua-womens-mh100-waterproof-mid-hiking-shoes-133726-133727",
    ]

    logger.info("Starting concurrent scraping with Playwright + Stealth...")
    asyncio.run(concurrent_scraping(urls, max_concurrent=1, max_retries=3))
    logger.info("Scraping complete.")