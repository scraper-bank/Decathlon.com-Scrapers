"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import undetected_chromedriver as uc
from seleniumwire import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import json
import re
import logging
import threading
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"decathlon_com_product_page_scraper_data_{timestamp}.jsonl"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    'proxy': {
        'http': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'https': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'no_proxy': 'localhost:127.0.0.1'
    }
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Thread-local storage for WebDriver instances
thread_local = threading.local()

@dataclass
class ScrapedData:
    url: str = ""
    productId: str = ""
    name: str = ""
    brand: str = ""
    category: str = ""
    description: str = ""
    price: float = 0.0
    preDiscountPrice: float = 0.0
    currency: str = "USD"
    availability: str = "in_stock"
    aggregateRating: Dict[str, Any] = field(default_factory=lambda: {
        "bestRating": 5, "ratingValue": 0.0, "reviewCount": 0, "worstRating": 1
    })
    images: List[Dict[str, str]] = field(default_factory=list)
    videos: List[Dict[str, str]] = field(default_factory=list)
    features: List[str] = field(default_factory=list)
    specifications: List[Dict[str, str]] = field(default_factory=list)
    reviews: List[Dict[str, Any]] = field(default_factory=list)
    serialNumbers: List[Dict[str, str]] = field(default_factory=list)
    seller: Dict[str, Any] = field(default_factory=lambda: {
        "name": "Decathlon", "rating": None, "url": "https://www.decathlon.com"
    })

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data):
        item_key = input_data.productId if input_data.productId else input_data.url
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def get_driver():
    """Get thread-local undetected ChromeDriver instance with ScrapeOps Residential Proxy."""
    if not hasattr(thread_local, "driver"):
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.notifications": 2,
        }
        
        options = uc.ChromeOptions()
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--window-size=1920,1080")
        options.add_experimental_option("prefs", prefs)
        options.page_load_strategy = 'eager'
        
        thread_local.driver = webdriver.Chrome(
            options=options,
            seleniumwire_options=PROXY_CONFIG
        )
    return thread_local.driver

def strip_html(text: str) -> str:
    return re.sub(r'<[^>]*>', ' ', text).strip()

def make_absolute_url(url_str: str) -> str:
    if not url_str: return ""
    if url_str.startswith(("http://", "https://")): return url_str
    if url_str.startswith("//"): return "https:" + url_str
    domain = "https://www.decathlon.com"
    return f"{domain}{url_str}" if url_str.startswith("/") else f"{domain}/{url_str}"

def detect_currency(price_text: str) -> str:
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR", "GBP": "GBP", "£": "GBP",
        "JPY": "JPY", "¥": "JPY", "CAD": "CAD", "AUD": "AUD"
    }
    for code, currency in currency_map.items():
        if code in price_text: return currency
    return "USD"

def parse_price(price_str: str) -> float:
    try:
        match = re.search(r'[\d,]+\.?\d*', price_str.replace(",", ""))
        return float(match.group()) if match else 0.0
    except: return 0.0

def clean_shopify_url(url: str) -> str:
    # Remove _400x400, _grande, etc.
    url = re.sub(r'(_\d+x\d+|_small|_thumb|_icon|_grande|_medium|_large)(\.[a-z]+)', r'\2', url)
    # Remove width query parameter
    url = re.sub(r'[?&]width=\d+', '', url)
    return url.rstrip('?')

def is_blacklisted(url: str) -> bool:
    lower = url.lower()
    blacklist = ["logo", "placeholder", "flag", "spicegems", "/assets/", "default_logo", "payment", "checkout", "icon"]
    return any(item in lower for item in blacklist)

def extract_data(driver: webdriver.Chrome, url: str) -> Optional[ScrapedData]:
    html_content = driver.page_source
    data = ScrapedData(url=url)
    
    # 1. JSON-LD and Variant Data Extraction
    json_ld = None
    scripts = driver.find_elements(By.CSS_SELECTOR, "script[type='application/ld+json']")
    for script in scripts:
        try:
            content = json.loads(script.get_attribute('text'))
            if isinstance(content, dict) and content.get("@type") in ["Product", "ProductGroup"]:
                json_ld = content
        except: continue

    variant_data = None
    try:
        v_script = driver.find_element(By.CSS_SELECTOR, "variant-selects script[type='application/json']")
        variant_data = json.loads(v_script.get_attribute('text'))
    except: pass

    # 2. Basic Info
    data.name = json_ld.get("name") if json_ld else ""
    if not data.name:
        try: data.name = driver.find_element(By.CSS_SELECTOR, ".product__title h1, h1").text.strip()
        except: pass

    data.brand = json_ld.get("brand", {}).get("name") if json_ld and isinstance(json_ld.get("brand"), dict) else ""
    if not data.brand:
        try: data.brand = driver.find_element(By.CSS_SELECTOR, ".product__text").text.strip()
        except: pass

    data.category = json_ld.get("category") if json_ld else ""
    if not data.category:
        try: data.category = driver.find_element(By.CSS_SELECTOR, ".dkt-level1__link").text.strip()
        except: pass

    try:
        desc_el = driver.find_element(By.CSS_SELECTOR, ".product__description")
        data.description = desc_el.text.strip()
    except:
        data.description = json_ld.get("description", "") if json_ld else ""

    # 3. Prices and Currency
    price_text = ""
    try:
        price_el = driver.find_element(By.CSS_SELECTOR, ".price-item--sale, .price-item--regular")
        price_text = price_el.text
        data.currency = detect_currency(price_text)
    except: pass

    if variant_data:
        data.price = variant_data.get("price", 0) / 100.0
        data.preDiscountPrice = variant_data.get("compare_at_price", 0) / 100.0
        if not variant_data.get("available", True):
            data.availability = "out_of_stock"
    
    if data.price == 0:
        data.price = parse_price(price_text)
    if data.preDiscountPrice == 0:
        try: data.preDiscountPrice = parse_price(driver.find_element(By.CSS_SELECTOR, ".price-item--regular").text)
        except: pass

    # 4. Product ID
    pid = ""
    try: pid = driver.find_element(By.CSS_SELECTOR, "product-info").get_attribute("data-product-id")
    except:
        try: pid = driver.find_element(By.CSS_SELECTOR, "variant-selects").get_attribute("data-product-id")
        except: pass
    
    if not pid:
        try:
            rid_match = re.search(r'"rid":\s*(\d+)', html_content)
            pid = rid_match.group(1) if rid_match else ""
        except: pass
    data.productId = pid

    # 5. Ratings
    rating_val = 0.0
    rev_count = 0
    if json_ld and "aggregateRating" in json_ld:
        agg = json_ld["aggregateRating"]
        rating_val = float(agg.get("ratingValue", 0))
        rev_count = int(agg.get("reviewCount", 0))
    
    if rating_val == 0:
        try: rating_val = float(driver.find_element(By.CSS_SELECTOR, ".rating-text span").text.strip())
        except: pass
    
    if rev_count == 0:
        try:
            count_text = driver.find_element(By.CSS_SELECTOR, ".rating-count").text
            rev_count = int(re.search(r'\d+', count_text.replace(",", "")).group())
        except: pass
    
    data.aggregateRating = {
        "bestRating": 5, "ratingValue": rating_val, "reviewCount": rev_count, "worstRating": 1
    }

    # 6. Images
    seen_urls = set()
    img_elements = driver.find_elements(By.CSS_SELECTOR, "media-gallery img")
    for img in img_elements:
        src = img.get_attribute("src") or img.get_attribute("data-src")
        if src:
            abs_url = clean_shopify_url(make_absolute_url(src))
            if abs_url and not is_blacklisted(abs_url) and abs_url not in seen_urls:
                data.images.append({"url": abs_url, "alt_text": img.get_attribute("alt") or ""})
                seen_urls.add(abs_url)

    # 7. Features
    feature_els = driver.find_elements(By.CSS_SELECTOR, "#featuresGrid .pdp-modal-features__item")
    for el in feature_els:
        try:
            txt = el.find_element(By.CSS_SELECTOR, ".pdp-modal-features__text").text.strip()
            if txt: data.features.append(txt)
        except: continue

    # 8. Specifications
    specs = []
    comp_items = driver.find_elements(By.CSS_SELECTOR, "#materialsGrid .composition-item")
    for item in comp_items:
        txt = item.text
        if ":" in txt:
            k, v = txt.split(":", 1)
            specs.append({"key": k.strip(), "value": v.strip()})
    
    last_key = ""
    spec_nodes = driver.find_elements(By.CSS_SELECTOR, "#specificationsGrid .pdp-modal-features__title, #specificationsGrid .pdp-modal-features__text")
    for node in spec_nodes:
        cls = node.get_attribute("class")
        if "title" in cls:
            last_key = node.text.strip()
        elif "text" in cls and last_key:
            val = node.text.strip()
            if val and "?" not in last_key and len(last_key) < 100:
                specs.append({"key": last_key, "value": val})
            last_key = ""
    data.specifications = specs

    # 9. Reviews
    review_items = driver.find_elements(By.CSS_SELECTOR, ".pdp-reviews__item")
    for item in review_items:
        try:
            meta_text = item.find_element(By.CSS_SELECTOR, ".pdp-reviews__review-meta").text
            author = meta_text.split("|")[0].strip()
            content = strip_html(item.find_element(By.CSS_SELECTOR, ".pdp-reviews__review-text").get_attribute("innerHTML"))
            title = item.find_element(By.CSS_SELECTOR, ".pdp-reviews__review-title").text.strip()
            
            date_match = re.search(r'(?i)(\d+\s+(day|week|month|year)s?\s+ago|less\s+than\s+a\s+month\s+ago)', meta_text)
            date_part = date_match.group(1) if date_match else ""

            rating = 5.0
            negative_indicators = ["disappointed", "wear and tear", "poor", "terrible", "worst"]
            if any(ind in (title + content).lower() for ind in negative_indicators):
                rating = 1.0
            
            data.reviews.append({
                "author": author, "content": content, "date": date_part, "rating": rating, "title": title
            })
        except: continue

    # 10. Videos
    seen_vids = set()
    v_matches = re.findall(r'https://scontent\.cdninstagram\.com/[^"\'\\s>]+', html_content)
    for v in v_matches:
        v_url = v.replace("\\u0026", "&").replace("&amp;", "&")
        if ".mp4" in v_url and v_url not in seen_vids:
            data.videos.append({"url": v_url})
            seen_vids.add(v_url)

    try: data.url = driver.find_element(By.CSS_SELECTOR, "link[rel='canonical']").get_attribute("href")
    except: pass

    return data

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    tries = 0
    success = False
    driver = get_driver()
    while tries <= retries and not success:
        try:
            driver.get(url)
            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            scraped_data = extract_data(driver, url)
            if scraped_data:
                pipeline.add_data(scraped_data)
                success = True
        except Exception as e:
            logger.error(f"Error scraping {url}: {e}")
            tries += 1
    if not success:
        logger.error(f"Failed {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            future.result()
    if hasattr(thread_local, "driver"):
        thread_local.driver.quit()

if __name__ == "__main__":
    urls = [
        "https://www.decathlon.com/products/quechua-womens-mh100-waterproof-mid-hiking-shoes-133726-133727",
    ]
    logger.info("Starting Scraper...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Done.")