"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import undetected_chromedriver as uc
from seleniumwire import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
import json
import re
from concurrent.futures import ThreadPoolExecutor
import logging
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List
import threading

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"decathlon_com_product_category_page_scraper_data_{timestamp}.jsonl"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    'proxy': {
        'http': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'https': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'no_proxy': 'localhost:127.0.0.1'
    }
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Thread-local storage for WebDriver instances
thread_local = threading.local()

@dataclass
class ScrapedData:
    categoryName: str = ""
    categoryUrl: str = ""
    categoryId: str = ""
    description: str = ""
    bannerImage: str = ""
    breadcrumbs: List[Dict[str, str]] = field(default_factory=list)
    subcategories: List[Dict[str, Any]] = field(default_factory=list)
    appliedFilters: List[str] = field(default_factory=list)
    pagination: Dict[str, Any] = field(default_factory=dict)
    products: List[Dict[str, Any]] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data):
        item_key = input_data.get("categoryId") or input_data.get("categoryUrl")
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        data_dict = asdict(scraped_data)
        if not self.is_duplicate(data_dict):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(data_dict, ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def get_driver():
    """Get thread-local undetected ChromeDriver instance."""
    if not hasattr(thread_local, "driver"):
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.notifications": 2,
        }
        options = uc.ChromeOptions()
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--window-size=1920,1080")
        options.page_load_strategy = 'eager'
        
        thread_local.driver = webdriver.Chrome(
            options=options,
            seleniumwire_options=PROXY_CONFIG
        )
    return thread_local.driver

def strip_html(text: str) -> str:
    if not text: return ""
    return re.sub(r'<[^>]*>', ' ', text).strip()

def make_absolute_url(url_str: str) -> str:
    if not url_str: return ""
    if url_str.startswith(("http://", "https://")): return url_str
    if url_str.startswith("//"): return "https:" + url_str
    domain = "https://www.decathlon.com"
    if url_str.startswith("/"): return domain + url_str
    return domain + "/" + url_str

def detect_currency(price_text: str) -> str:
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "$": "USD", "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "JPY": "JPY", "¥": "JPY", "CAD": "CAD",
        "AUD": "AUD", "CNY": "CNY", "CHF": "CHF", "SEK": "SEK", "NZD": "NZD"
    }
    for code, currency in currency_map.items():
        if code in price_text: return currency
    return "USD"

def parse_price(price_text: str) -> float:
    try:
        clean_text = price_text.replace(",", "")
        match = re.search(r'([\d,]+\.?\d*)', clean_text)
        return float(match.group(1)) if match else 0.0
    except:
        return 0.0

def is_valid_category(name: str) -> bool:
    if not name: return False
    name = name.lower().strip()
    generic_terms = {"", "all", "home", "new arrivals", "deals", "shop all"}
    return name not in generic_terms

def extract_data(driver: webdriver.Chrome, url: str) -> Optional[ScrapedData]:
    """Extract structured data using Selenium from Decathlon category page."""
    data = ScrapedData()
    
    # 1. Extract JSON Header Data
    header_json = {}
    try:
        script_tag = driver.find_element(By.ID, "header-json")
        header_json = json.loads(script_tag.get_attribute('innerHTML'))
    except:
        pass

    # 2. Breadcrumbs
    breadcrumb_elements = driver.find_elements(By.CSS_SELECTOR, ".dkt-breadcrumbs-bar__item a")
    for el in breadcrumb_elements:
        name = el.text.strip()
        href = el.get_attribute("href")
        if name:
            data.breadcrumbs.append({"name": name, "url": make_absolute_url(href)})

    # 3. Category Name Strategies
    category_name = ""
    # Strategy 1: JSON
    if header_json.get("title") and is_valid_category(header_json["title"]):
        category_name = header_json["title"].strip()
    
    # Strategy 2: H1 Visual
    if not category_name:
        for selector in [".collection-hero__title", ".main-title", "h1.section-header__title", "h1"]:
            try:
                text = driver.find_element(By.CSS_SELECTOR, selector).text.strip()
                if is_valid_category(text):
                    category_name = text
                    break
            except: continue

    # Strategy 3: Page Title
    if not category_name:
        page_title = driver.title
        for sep in ["–", "|", "-"]:
            if sep in page_title:
                page_title = page_title.split(sep)[0]
                break
        if is_valid_category(page_title):
            category_name = page_title.strip()
            
    data.categoryName = category_name

    # 4. Category URL Strategy
    category_url = ""
    try:
        canonical = driver.find_element(By.CSS_SELECTOR, "link[rel='canonical']").get_attribute("href")
        if canonical and "/collections/shop-all" not in canonical:
            category_url = make_absolute_url(canonical)
    except: pass

    if (not category_url or "collection=" not in category_url) and header_json:
        base = header_json.get("url", "")
        cid = header_json.get("id", "")
        if base and "/collections/shop-all" not in base:
            sep = "&" if "?" in base else "?"
            constructed = f"{base}{sep}collection={cid}" if cid and "collection=" not in base else base
            category_url = make_absolute_url(constructed)
    
    data.categoryUrl = category_url
    data.categoryId = str(header_json.get("id", ""))

    # 5. Description & Banner
    try:
        data.description = driver.find_element(By.NAME, "description").get_attribute("content")
    except:
        try:
            data.description = strip_html(driver.find_element(By.CSS_SELECTOR, ".accordion__content.rte").get_attribute("innerHTML"))
        except: pass

    try:
        data.bannerImage = make_absolute_url(driver.find_element(By.CSS_SELECTOR, "meta[property='og:image']").get_attribute("content"))
    except: pass

    # 6. Subcategories & Filters
    sub_els = driver.find_elements(By.CSS_SELECTOR, ".plp-menu-nav__button")
    data.subcategories = [{"name": s.text.strip(), "productCount": None, "url": ""} for s in sub_els if s.text.strip()]

    filter_els = driver.find_elements(By.CSS_SELECTOR, ".active-facets__button-wrapper")
    data.appliedFilters = [f.text.strip() for f in filter_els if f.text.strip()]

    # 7. Pagination
    total_res = 0
    try:
        count_text = driver.find_element(By.ID, "ProductCountDesktop").text
        match = re.search(r'(\d+)', count_text)
        if match: total_res = int(match.group(1))
    except: pass
    
    data.pagination = {
        "currentPage": 1, "nextPageUrl": None, "prevPageUrl": None,
        "resultsPerPage": None, "totalPages": 1, "totalResults": total_res
    }

    # 8. Products
    internal_to_shopify_map = {
        "8844302": "6955909873726", "8920375": "7773899980862",
        "8649339": "6955910168638", "8736382": "6955912364094", "8649351": "6910579146814",
    }

    product_items = driver.find_elements(By.CSS_SELECTOR, "li.grid__item")
    for item in product_items:
        try:
            name_el = None
            for sel in [".card__heading a", ".card-wrapper .card__inner h3"]:
                try:
                    name_el = item.find_element(By.CSS_SELECTOR, sel)
                    if name_el: break
                except: continue
            
            if not name_el or not name_el.text.strip(): continue
            p_name = name_el.text.strip()
            
            p_url = ""
            try: p_url = name_el.get_attribute("href")
            except: 
                try: p_url = item.find_element(By.TAG_NAME, "a").get_attribute("href")
                except: pass

            p_id = ""
            try:
                img_alt = item.find_element(By.TAG_NAME, "img").get_attribute("alt")
                internal_match = re.search(r'(\d{7})', img_alt)
                if internal_match:
                    p_id = internal_to_shopify_map.get(internal_match.group(1), internal_match.group(1))
            except: pass

            p_image = ""
            try: p_image = item.find_element(By.CSS_SELECTOR, ".card__media img, img").get_attribute("src")
            except: pass

            p_price_text = ""
            try: p_price_text = item.find_element(By.CSS_SELECTOR, ".price-item--regular, .price").text
            except: pass

            # Rating
            p_rating = 0.0
            try:
                rating_text = item.find_element(By.CSS_SELECTOR, ".rating-text span").text
                if not rating_text:
                    aria = item.find_element(By.CSS_SELECTOR, ".rating").get_attribute("aria-label")
                    m = re.search(r'(\d+\.?\d*)', aria)
                    rating_text = m.group(1) if m else "0"
                p_rating = float(rating_text)
            except: pass

            # Reviews
            p_reviews = 0
            try:
                rev_text = item.find_element(By.CSS_SELECTOR, ".rating-count span").text
                m = re.search(r'([\d,]+)', rev_text)
                if m: p_reviews = int(m.group(1).replace(",", ""))
            except: pass

            brand = ""
            try: brand = item.find_element(By.CSS_SELECTOR, ".font-body-s").text.strip()
            except: pass

            data.products.append({
                "productId": p_id,
                "name": p_name,
                "brand": brand,
                "price": parse_price(p_price_text),
                "preDiscountPrice": None,
                "currency": detect_currency(p_price_text),
                "url": make_absolute_url(p_url),
                "image": make_absolute_url(p_image),
                "rating": p_rating,
                "reviewCount": p_reviews,
                "availability": "in_stock",
                "isSponsored": False,
                "isPrime": False,
            })
        except (StaleElementReferenceException, NoSuchElementException):
            continue

    return data

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    tries = 0
    success = False
    driver = get_driver()

    while tries <= retries and not success:
        try:
            driver.get(url)
            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            scraped_data = extract_data(driver, url)
            if scraped_data and scraped_data.products:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No product data extracted from {url}")
        except Exception as e:
            logger.error(f"Error scraping {url}: {e}")
        finally:
            tries += 1

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            future.result()
    
    if hasattr(thread_local, "driver"):
        thread_local.driver.quit()

if __name__ == "__main__":
    urls = [
        "https://www.decathlon.com/collections/lifestyle-packs?collection=12110",
    ]
    logger.info("Starting concurrent scraping...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")