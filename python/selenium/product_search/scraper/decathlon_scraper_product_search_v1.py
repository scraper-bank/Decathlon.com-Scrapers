"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import undetected_chromedriver as uc
from seleniumwire import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
import json
import re
import logging
import threading
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"decathlon_com_product_search_page_scraper_data_{timestamp}.jsonl"

# ScrapeOps Residential Proxy Configuration
PROXY_CONFIG = {
    'proxy': {
        'http': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'https': f'http://scrapeops:{API_KEY}@residential-proxy.scrapeops.io:8181',
        'no_proxy': 'localhost:127.0.0.1'
    }
}

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Thread-local storage for WebDriver instances
thread_local = threading.local()

@dataclass
class ScrapedData:
    breadcrumbs: Optional[List[Any]] = None
    pagination: Dict[str, Any] = field(default_factory=dict)
    products: List[Dict[str, Any]] = field(default_factory=list)
    recommendations: Optional[Any] = None
    relatedSearches: List[str] = field(default_factory=list)
    searchMetadata: Dict[str, Any] = field(default_factory=dict)
    sponsoredProducts: List[Any] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data):
        # Use productId as a unique key if available
        products = input_data.get('products', [])
        if not products:
            return False
        
        # Simple duplicate check based on the first product ID in the batch
        item_key = products[0].get('productId', str(input_data))
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        data_dict = asdict(scraped_data)
        if not self.is_duplicate(data_dict):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(data_dict, ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved batch to {self.jsonl_filename}")

def get_driver():
    """Get thread-local undetected ChromeDriver instance."""
    if not hasattr(thread_local, "driver"):
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.notifications": 2,
        }
        
        options = uc.ChromeOptions()
        options.add_argument("--headless=new")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--window-size=1920,1080")
        
        options.add_experimental_option("prefs", prefs)
        options.page_load_strategy = 'eager'
        
        thread_local.driver = webdriver.Chrome(
            options=options,
            seleniumwire_options=PROXY_CONFIG
        )
        
    return thread_local.driver

def make_absolute_url(url_str: str, domain: str = "https://www.decathlon.com") -> str:
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return "https:" + url_str
    if url_str.startswith("/"):
        return domain + url_str
    return f"{domain}/{url_str}"

def detect_currency(price_text: str) -> str:
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP",
        "JPY": "JPY", "¥": "JPY",
        "CAD": "CAD", "CA$": "CAD",
        "AUD": "AUD", "AU$": "AUD",
        "CNY": "CNY", "CN¥": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def parse_price(price_text: str) -> float:
    try:
        cleaned = price_text.replace(",", "")
        match = re.search(r'[\d,]+\.?\d*', cleaned)
        if match:
            return float(match.group())
    except (ValueError, TypeError):
        pass
    return 0.0

def extract_data(driver: webdriver.Chrome, url: str) -> Optional[ScrapedData]:
    """Extract structured data using Selenium from the Decathlon search page."""
    domain = "https://www.decathlon.com"
    products = []
    
    try:
        items = driver.find_elements(By.CSS_SELECTOR, "li.grid__item")
        for item in items:
            try:
                # Name extraction with fallbacks
                name = ""
                name_selectors = [".card__heading a.full-unstyled-link", ".card__heading.h5 a", ".card__heading a", "h3 a"]
                for sel in name_selectors:
                    els = item.find_elements(By.CSS_SELECTOR, sel)
                    if els:
                        name = els[0].text.strip()
                        if name: break
                
                if not name:
                    continue

                # URL
                product_url = ""
                link_els = item.find_elements(By.CSS_SELECTOR, ".card__heading a")
                if link_els:
                    href = link_els[0].get_attribute("href")
                    product_url = make_absolute_url(href)

                # Pricing
                price = 0.0
                pre_discount_price = None
                currency = "USD"
                
                price_els = item.find_elements(By.CSS_SELECTOR, ".price-item--sale, .price-item--regular")
                if price_els:
                    price_text = price_els[0].text.strip()
                    price = parse_price(price_text)
                    currency = detect_currency(price_text)

                was_els = item.find_elements(By.CSS_SELECTOR, ".price__sale .price-item--regular")
                if was_els:
                    was_text = was_els[0].text.strip()
                    if was_text:
                        pre_discount_price = parse_price(was_text)

                # Brand
                brand = "Quechua"
                brand_els = item.find_elements(By.CSS_SELECTOR, ".font-body-s")
                if brand_els and brand_els[0].text.strip():
                    brand = brand_els[0].text.strip()

                # Ratings
                rating_value = 0.0
                review_count = 0
                rating_els = item.find_elements(By.CSS_SELECTOR, ".rating-text span")
                if rating_els:
                    try:
                        rating_value = float(rating_els[0].text.strip())
                    except ValueError: pass
                
                count_els = item.find_elements(By.CSS_SELECTOR, ".rating-count span")
                if count_els:
                    count_text = count_els[0].text.replace(",", "")
                    match = re.search(r'\d+', count_text)
                    if match:
                        review_count = int(match.group())

                # Images
                images = []
                img_els = item.find_elements(By.CSS_SELECTOR, ".card__media img")
                if img_els:
                    img = img_els[0]
                    images.append({
                        "url": make_absolute_url(img.get_attribute("src")),
                        "altText": img.get_attribute("alt")
                    })

                # Badges & Promotions
                badges = []
                badge_els = item.find_elements(By.CSS_SELECTOR, ".card__badge .badge, .card__badge span")
                seen_labels = set()
                for b in badge_els:
                    label = b.text.strip()
                    if label and label not in seen_labels:
                        badges.append({"imageUrl": None, "label": label, "type": "deal"})
                        seen_labels.add(label)

                promotions = []
                for b in badges:
                    label = b["label"]
                    discount = 0.0
                    match = re.search(r'(\d+)%', label)
                    if match:
                        discount = float(match.group(1))
                    promotions.append({
                        "description": label,
                        "discountPercentage": discount,
                        "endDate": None,
                        "type": "discount"
                    })

                # Variants
                visible_options = []
                swatches = item.find_elements(By.CSS_SELECTOR, ".swatch")
                for sw in swatches:
                    val = sw.get_attribute("data-variant-title")
                    img_src = sw.get_attribute("data-variant-image-src")
                    if val:
                        visible_options.append({
                            "imageUrl": make_absolute_url(img_src),
                            "type": "color/size",
                            "value": val
                        })

                # Product ID Strategies
                product_id = ""
                # Strategy 1
                heading_link = item.find_elements(By.CSS_SELECTOR, ".card__heading a")
                if heading_link:
                    id_attr = heading_link[0].get_attribute("id")
                    if id_attr and "--" in id_attr:
                        product_id = id_attr.split("--")[-1]
                
                # Strategy 2
                if not product_id:
                    modal = item.find_elements(By.CSS_SELECTOR, "modal-opener")
                    if modal:
                        modal_data = modal[0].get_attribute("data-modal")
                        match = re.search(r'\d+', modal_data)
                        if match: product_id = match.group()

                # Strategy 3
                if not product_id:
                    heading = item.find_elements(By.CSS_SELECTOR, ".card__heading")
                    if heading:
                        id_attr = heading[0].get_attribute("id")
                        if id_attr and "--" in id_attr:
                            product_id = id_attr.split("--")[-1]

                product = {
                    "aggregateRating": {
                        "bestRating": 5,
                        "ratingValue": rating_value,
                        "reviewCount": review_count,
                        "worstRating": 1,
                    },
                    "availability": "in_stock",
                    "availabilityMessage": None,
                    "badges": badges,
                    "brand": brand,
                    "category": "Shoes",
                    "currency": currency,
                    "description": None,
                    "features": [],
                    "images": images,
                    "name": name,
                    "preDiscountPrice": pre_discount_price,
                    "price": price,
                    "productId": product_id,
                    "reviews": [],
                    "seller": {
                        "name": "Decathlon",
                        "rating": None,
                        "url": domain,
                    },
                    "serialNumbers": [],
                    "specifications": [],
                    "url": product_url,
                    "videos": [],
                    "promotions": promotions,
                    "variants": {
                        "variantCount": None,
                        "visibleOptions": visible_options,
                    },
                }
                products.append(product)
            except (StaleElementReferenceException, NoSuchElementException) as e:
                logger.debug(f"Skipping an item due to element error: {e}")
                continue

        # Total Results
        total_results = 0
        try:
            count_el = driver.find_element(By.CSS_SELECTOR, "#ProductCountDesktop")
            match = re.search(r'\d+', count_el.text)
            if match:
                total_results = int(match.group())
        except NoSuchElementException:
            pass

        return ScrapedData(
            products=products,
            pagination={
                "currentPage": 1,
                "hasNextPage": False,
                "hasPreviousPage": False,
                "nextPageUrl": None,
                "previousPageUrl": None,
                "totalPages": 1,
            },
            searchMetadata={
                "query": "kids shoes",
                "resultsDisplayed": len(products),
                "searchType": "keyword",
                "searchUrl": url,
                "totalResults": total_results,
            }
        )
    except Exception as e:
        logger.error(f"Extraction error: {e}")
        return None

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic."""
    tries = 0
    success = False
    driver = get_driver()

    while tries <= retries and not success:
        try:
            driver.get(url)
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "li.grid__item"))
            )
            scraped_data = extract_data(driver, url)
            if scraped_data and scraped_data.products:
                pipeline.add_data(scraped_data)
                success = True
            else:
                logger.warning(f"No products extracted from {url} on try {tries+1}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            future.result()
    
    if hasattr(thread_local, "driver"):
        thread_local.driver.quit()

if __name__ == "__main__":
    urls = [
        "https://www.decathlon.com/search?q=kids+shoes&options%5Bprefix%5D=last",
    ]

    logger.info("Starting concurrent scraping with undetected ChromeDriver + Selenium Wire...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")