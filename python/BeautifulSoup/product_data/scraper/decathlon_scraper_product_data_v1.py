"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlencode, urljoin
import json
import re
from concurrent.futures import ThreadPoolExecutor
import logging
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"decathlon_com_product_page_scraper_data_{timestamp}.jsonl"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    aggregateRating: Dict[str, Any] = field(default_factory=dict)
    availability: str = "in_stock"
    brand: str = ""
    category: str = ""
    currency: str = "USD"
    description: str = ""
    features: List[str] = field(default_factory=list)
    images: List[Dict[str, str]] = field(default_factory=list)
    name: str = ""
    preDiscountPrice: Optional[float] = None
    price: Optional[float] = None
    productId: str = ""
    reviews: List[Dict[str, Any]] = field(default_factory=list)
    seller: Dict[str, Any] = field(default_factory=lambda: {
        "name": "Decathlon",
        "rating": None,
        "url": "https://www.decathlon.com"
    })
    serialNumbers: List[Dict[str, str]] = field(default_factory=list)
    specifications: List[Dict[str, str]] = field(default_factory=list)
    url: str = ""
    videos: List[Dict[str, str]] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData):
        item_key = input_data.productId or input_data.url
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def strip_html(text: str) -> str:
    if not text:
        return ""
    re_tag = re.compile(r"<[^>]*>")
    return re.sub(re_tag, " ", text).strip()

def make_absolute_url(url_str: str) -> str:
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return f"https:{url_str}"
    domain = "https://www.decathlon.com"
    return urljoin(domain, url_str)

def detect_currency(price_text: str) -> str:
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def clean_shopify_url(raw_url: str) -> str:
    if not raw_url:
        return ""
    if raw_url.startswith("//"):
        raw_url = "https:" + raw_url
    return re.sub(r"[&?]width=\d+", "", raw_url)

def extract_data(soup: BeautifulSoup) -> Optional[ScrapedData]:
    """Extract structured data from HTML using BeautifulSoup."""
    try:
        json_product = None
        for script in soup.find_all("script", type="application/ld+json"):
            try:
                data = json.loads(script.string)
                if isinstance(data, dict):
                    obj_type = data.get("@type")
                    if obj_type in ["Product", "ProductGroup"]:
                        json_product = data
                        break
            except (json.JSONDecodeError, TypeError):
                continue

        variant_data = None
        variant_script = soup.select_one("variant-selects script[type='application/json']")
        if variant_script:
            try:
                variant_data = json.loads(variant_script.string)
            except (json.JSONDecodeError, TypeError):
                pass

        data_obj = ScrapedData()

        # 1. aggregateRating
        rating_map = {"bestRating": 5, "worstRating": 1}
        rating_el = soup.select_one(".rating")
        rating_text = rating_el.get("aria-label", "") if rating_el else ""
        rating_match = re.search(r"([\d.]+)", rating_text)
        
        if rating_match:
            rating_map["ratingValue"] = float(rating_match.group(1))
        elif json_product and "aggregateRating" in json_product:
            rating_map["ratingValue"] = json_product["aggregateRating"].get("ratingValue")
        
        if rating_map.get("ratingValue") is None:
            r_val = soup.select_one("#averageRating")
            if r_val and r_val.get_text(strip=True):
                try:
                    rating_map["ratingValue"] = float(r_val.get_text(strip=True))
                except ValueError:
                    pass

        count_text = soup.select_one(".rating-count")
        if count_text:
            count_match = re.search(r"[\d,]+", count_text.get_text().replace(",", ""))
            if count_match:
                rating_map["reviewCount"] = int(count_match.group(0))
        
        data_obj.aggregateRating = rating_map

        # 2. availability
        if variant_data and variant_data.get("available") is False:
            data_obj.availability = "out_of_stock"

        # 3. brand
        brand_name = ""
        if json_product and isinstance(json_product.get("brand"), dict):
            brand_name = json_product["brand"].get("name", "")
        if not brand_name:
            brand_el = soup.select_one(".product__text.font-body-s")
            brand_name = brand_el.get_text(strip=True) if brand_el else ""
        data_obj.brand = brand_name

        # 4. category
        cat_name = json_product.get("category", "") if json_product else ""
        if not cat_name:
            cat_el = soup.select(".dkt-breadcrumbs-bar__item")
            cat_name = cat_el[-1].get_text(strip=True) if cat_el else ""
        data_obj.category = cat_name

        # 5. currency
        og_currency = soup.find("meta", property="og:price:currency")
        if og_currency:
            data_obj.currency = og_currency.get("content", "USD")
        else:
            price_box = soup.select_one(".price")
            data_obj.currency = detect_currency(price_box.get_text() if price_box else "")

        # 6. description
        desc_el = soup.select_one(".product__description")
        desc_text = desc_el.get_text(strip=True) if desc_el else ""
        if not desc_text and json_product:
            desc_text = json_product.get("description", "")
        data_obj.description = desc_text

        # 7. features
        features = []
        if json_product and "description" in json_product:
            desc_val = json_product["description"]
            if "Highlights:" in desc_val:
                found_highlights = False
                for line in desc_val.split("\n"):
                    line = line.strip()
                    if not line: continue
                    if "Highlights:" in line:
                        found_highlights = True
                        parts = line.split("Highlights:", 1)
                        if len(parts) > 1 and parts[1].strip():
                            features.append(parts[1].strip())
                        continue
                    if found_highlights:
                        features.append(line)
        
        if not features:
            for item in soup.select("#featuresGrid .pdp-modal-features__item"):
                title = item.select_one(".pdp-modal-features__title")
                text = item.select_one(".pdp-modal-features__text")
                t_str = title.get_text(strip=True) if title else ""
                x_str = text.get_text(strip=True) if text else ""
                if t_str and x_str:
                    features.append(f"{t_str}: {x_str}")
                elif x_str:
                    features.append(x_str)
        data_obj.features = features

        # 8. images
        images = []
        seen_urls = set()
        context_photo_count = 0
        
        img_els = soup.select("product-info .product__media-list img, product-modal img")
        for img in img_els:
            src = img.get("src") or img.get("data-src")
            if not src or "placeholder" in src:
                continue
            
            full_url = clean_shopify_url(src)
            if not full_url or full_url in seen_urls:
                continue
            
            alt = img.get("alt", "")
            is_context = "[[contextphoto]]" in alt
            if is_context:
                if context_photo_count >= 2:
                    continue
                clean_alt = "[[contextphoto]]"
                context_photo_count += 1
            else:
                clean_alt = alt.replace("{*", "").replace("*}", "").strip()
            
            seen_urls.add(full_url)
            images.append({"url": full_url, "altText": clean_alt})

        if not images and json_product and "image" in json_product:
            raw_imgs = json_product["image"]
            if isinstance(raw_imgs, str):
                raw_imgs = [raw_imgs]
            for r in raw_imgs:
                f_url = clean_shopify_url(r)
                if f_url and f_url not in seen_urls:
                    seen_urls.add(f_url)
                    images.append({"url": f_url, "altText": ""})
        data_obj.images = images

        # 9. name
        name_el = soup.find("h1")
        name_text = name_el.get_text(strip=True) if name_el else ""
        if not name_text and json_product:
            name_text = json_product.get("name", "")
        data_obj.name = name_text

        # 10 & 11. Prices
        if variant_data:
            cp = variant_data.get("compare_at_price")
            if isinstance(cp, (int, float)) and cp > 0:
                data_obj.preDiscountPrice = cp / 100.0
            
            p = variant_data.get("price")
            if isinstance(p, (int, float)):
                data_obj.price = p / 100.0

        if data_obj.preDiscountPrice is None:
            reg_el = soup.select_one(".price-item--regular")
            if reg_el:
                m = re.search(r"[\d.]+", reg_el.get_text().replace(",", ""))
                if m: data_obj.preDiscountPrice = float(m.group(0))

        if data_obj.price is None:
            sale_el = soup.select_one(".price-item--sale")
            if sale_el:
                m = re.search(r"[\d.]+", sale_el.get_text().replace(",", ""))
                if m: data_obj.price = float(m.group(0))

        # 12. productId
        prod_info = soup.find("product-info")
        p_id = prod_info.get("data-product-id", "") if prod_info else ""
        if not p_id and json_product:
            p_id = json_product.get("productGroupID", "")
        data_obj.productId = str(p_id)

        # 13. reviews
        reviews = []
        for rev in soup.select(".pdp-reviews__item"):
            r_title = rev.select_one(".pdp-reviews__review-title")
            r_content = rev.select_one(".pdp-reviews__review-text")
            r_author_line = rev.select_one(".pdp-reviews__author")
            
            title = r_title.get_text(strip=True) if r_title else ""
            content = r_content.get_text(strip=True) if r_content else ""
            author = r_author_line.get_text(strip=True).split("|")[0].strip() if r_author_line else ""
            
            date = ""
            for span in rev.find_all("span"):
                txt = span.get_text(strip=True)
                ltxt = txt.lower()
                if any(k in ltxt for k in ["ago", "month", "year", "day"]):
                    date = txt
                    break
            if not date:
                last_span = rev.select("span")
                if last_span:
                    date = last_span[-1].get_text(strip=True).lstrip("|").strip()

            review_rating = 0.0
            # Strategies for rating
            meta_rating = rev.select_one("[data-rating], [data-score], meta[itemprop='ratingValue']")
            if meta_rating:
                val = meta_rating.get("data-rating") or meta_rating.get("data-score") or meta_rating.get("content")
                try: review_rating = float(val)
                except: pass
            
            if review_rating == 0:
                rating_container = rev.select_one(".rating, [class*='rating'], [aria-label*='star'], .pdp-reviews__rating")
                if rating_container and rating_container.get("aria-label"):
                    m = re.search(r"([\d.]+)", rating_container["aria-label"])
                    if m: review_rating = float(m.group(1))

            if review_rating == 0:
                stars = len(rev.select(".icon-star.active, .filled-star, .spr-icon-star"))
                if stars > 0: review_rating = float(stars)

            if review_rating == 0:
                combined = (title + " " + content).lower()
                neg = ["disappointed", "poor", "bad", "worst", "terrible", "waste", "return", "broken", "wear and tear"]
                pos = ["great", "excellent", "perfect", "love", "comfortable", "good", "matches", "description", "nice"]
                if any(w in combined for w in neg): review_rating = 1.0
                elif any(w in combined for w in pos): review_rating = 5.0
            
            if review_rating == 0: review_rating = 5.0

            reviews.append({
                "author": author,
                "content": content,
                "date": date,
                "rating": review_rating,
                "title": title
            })
        data_obj.reviews = reviews

        # 15. serialNumbers
        serials = []
        if variant_data:
            if variant_data.get("sku"):
                serials.append({"type": "SKU", "value": str(variant_data["sku"])})
            if variant_data.get("barcode"):
                serials.append({"type": "GTIN", "value": str(variant_data["barcode"])})
        
        md_code = soup.select_one("#pdpModelCode")
        if md_code:
            val = md_code.get_text(strip=True).replace("ID ", "")
            serials.append({"type": "Other", "value": val})
        data_obj.serialNumbers = serials

        # 16. specifications
        specs = []
        for item in soup.select(".composition-item"):
            key_el = item.select_one(".font-overline")
            if key_el:
                key = key_el.get_text(strip=True)
                full_txt = item.get_text(strip=True)
                val = full_txt.replace(key, "").strip(": ")
                specs.append({"key": key.strip(":"), "value": val})
        
        for item in soup.select("#featuresGrid .pdp-modal-features__item, #specificationsGrid .pdp-modal-features__item"):
            k_el = item.select_one(".pdp-modal-features__title")
            v_el = item.select_one(".pdp-modal-features__text")
            if k_el and v_el:
                specs.append({
                    "key": k_el.get_text(strip=True),
                    "value": v_el.get_text(strip=True)
                })
        data_obj.specifications = specs

        # 17. url
        p_url = json_product.get("url", "") if json_product else ""
        if not p_url:
            can_el = soup.find("link", rel="canonical")
            p_url = can_el.get("href", "") if can_el else ""
        data_obj.url = make_absolute_url(p_url)

        # 18. videos
        videos = []
        for vid in soup.find_all("video"):
            v_url = vid.get("src", "")
            if v_url:
                videos.append({"url": v_url})
        data_obj.videos = videos

        return data_obj

    except Exception as e:
        logger.error(f"Error extracting data: {e}")
        return None

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic."""
    payload = {
        "api_key": API_KEY,
        "url": url,
        "optimize_request": True,
    }

    tries = 0
    success = False

    while tries <= retries and not success:
        try:
            proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)
            response = requests.get(proxy_url, timeout=30)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "lxml")
                scraped_data = extract_data(soup)
                
                if scraped_data:
                    pipeline.add_data(scraped_data)
                    success = True
                else:
                    logger.warning(f"No data extracted from {url}")
            else:
                logger.warning(f"Request failed for {url} with status {response.status_code}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            future.result()

if __name__ == "__main__":
    urls = [
        "https://www.decathlon.com/products/quechua-womens-mh100-waterproof-mid-hiking-shoes-133726-133727",
    ]

    logger.info("Starting concurrent scraping...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")