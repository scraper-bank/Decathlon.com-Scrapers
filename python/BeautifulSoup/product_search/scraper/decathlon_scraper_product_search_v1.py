"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import requests
import re
import json
import logging
from bs4 import BeautifulSoup
from urllib.parse import urlencode, urljoin
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List, Union

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"decathlon_com_product_search_page_scraper_data_{timestamp}.jsonl"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    breadcrumbs: Any = None
    pagination: Any = None
    products: List[Dict[str, Any]] = field(default_factory=list)
    recommendations: Any = None
    relatedSearches: Any = None
    searchMetadata: Dict[str, Any] = field(default_factory=lambda: {"query": "kids shoes"})
    sponsoredProducts: List[Any] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data: ScrapedData) -> bool:
        # Using a string representation of the products list as a simple uniqueness check
        item_key = hash(json.dumps([p.get("productId") for p in input_data.products], sort_keys=True))
        if item_key in self.items_seen:
            logger.warning(f"Duplicate content found. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        if not self.is_duplicate(scraped_data):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(asdict(scraped_data), ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved data to {self.jsonl_filename}")

def extract_data(soup: BeautifulSoup) -> Optional[ScrapedData]:
    """Extract structured data from HTML using BeautifulSoup, replicating Go logic."""
    domain = "https://www.decathlon.com"

    def make_absolute_url(url_str: str) -> str:
        if not url_str:
            return ""
        if url_str.startswith(("http://", "https://")):
            return url_str
        if url_str.startswith("//"):
            return "https:" + url_str
        if url_str.startswith("/"):
            return domain + url_str
        return f"{domain}/{url_str}"

    def detect_currency(price_text: str) -> str:
        price_text = price_text.upper()
        currency_map = {
            "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
            "EUR": "EUR", "€": "EUR",
            "GBP": "GBP", "£": "GBP", "GB£": "GBP",
            "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
            "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
            "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
            "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
            "CHF": "CHF", "FR.": "CHF",
            "SEK": "SEK", "KR": "SEK",
            "NZD": "NZD", "NZ$": "NZD",
        }
        for code, currency in currency_map.items():
            if code in price_text:
                return currency
        return "USD"

    def parse_price(price_str: str) -> float:
        if not price_str:
            return 0.0
        price_str = price_str.replace(",", "")
        match = re.search(r'[\d,]+\.?\d*', price_str)
        if match:
            try:
                return float(match.group())
            except ValueError:
                return 0.0
        return 0.0

    products = []
    
    # Matching doc.Find("li.grid__item")
    items = soup.select("li.grid__item")
    for s in items:
        p = {}

        # Product ID extraction logic
        p_id = ""
        id_attr = ""
        
        # Selectors matching Go logic for ID attribute
        link_el = s.select_one("a.full-unstyled-link")
        heading_link = s.select_one("h3.card__heading a")
        heading_el = s.select_one("h3.card__heading")

        if link_el and link_el.get("id"):
            id_attr = link_el.get("id")
        elif heading_link and heading_link.get("id"):
            id_attr = heading_link.get("id")
        elif heading_el and heading_el.get("id"):
            id_attr = heading_el.get("id")

        if id_attr:
            # Look for 10+ digits
            matches = re.search(r'\d{10,}', id_attr)
            if matches:
                p_id = matches.group()
            else:
                # Fallback to any digits
                digit_match = re.search(r'\d+', id_attr)
                if digit_match:
                    p_id = digit_match.group()

        # Final fallback to href
        if not p_id and link_el and link_el.get("href"):
            href_id = re.search(r'\d{5,}', link_el.get("href"))
            if href_id:
                p_id = href_id.group()

        # Name
        name_el = s.select_one(".card__heading a")
        name = name_el.get_text(strip=True) if name_el else ""

        # Brand
        brand_el = s.select_one(".font-body-s")
        brand = brand_el.get_text(strip=True) if brand_el else ""

        # Prices
        curr_price_el = s.select_one(".price-item--sale, .price-item--regular")
        curr_price_text = curr_price_el.get_text(strip=True) if curr_price_el else ""
        price = parse_price(curr_price_text)
        currency = detect_currency(curr_price_text)

        orig_price_el = s.select_one(".price__sale .price-item--regular")
        orig_price_text = orig_price_el.get_text(strip=True) if orig_price_el else ""
        pre_price = parse_price(orig_price_text) if orig_price_text else None

        # URL
        href = link_el.get("href") if link_el else ""
        prod_url = make_absolute_url(href)

        # Rating
        rating_val = 0.0
        r_text_el = s.select_one(".rating-text span")
        if r_text_el:
            try:
                rating_val = float(r_text_el.get_text(strip=True))
            except ValueError:
                pass
        
        review_count = 0
        rc_text_el = s.select_one(".rating-count span")
        if rc_text_el:
            rc_text = rc_text_el.get_text(strip=True).replace(",", "")
            rc_match = re.search(r'\d+', rc_text)
            if rc_match:
                review_count = int(rc_match.group())

        # Image
        img_el = s.find("img")
        img_src = img_el.get("src") if img_el else ""
        img_alt = img_el.get("alt") if img_el else ""

        # Badges & Promotions
        badges = []
        promos = []
        seen_badges = set()
        
        for badge_el in s.select(".badge"):
            raw_label = badge_el.get_text()
            clean_label = " ".join(raw_label.split())
            
            if clean_label and clean_label not in seen_badges:
                seen_badges.add(clean_label)
                badges.append({
                    "label": clean_label,
                    "type": "deal",
                })

                pct = 0
                pct_match = re.search(r'(\d+)%', clean_label)
                if pct_match:
                    pct = int(pct_match.group(1))

                promos.append({
                    "description": clean_label,
                    "discountPercentage": pct,
                    "type": "discount",
                    "endDate": None,
                })

        # Variants
        v_options = []
        for swatch in s.select(".swatch"):
            v_val = swatch.get("data-variant-title", "")
            v_img = swatch.get("data-variant-image-src", "")
            v_options.append({
                "type": "color/size",
                "value": v_val,
                "imageUrl": make_absolute_url(v_img),
            })

        p = {
            "productId": p_id,
            "name": name,
            "brand": brand,
            "price": price,
            "preDiscountPrice": pre_price,
            "currency": currency,
            "url": prod_url,
            "images": [{"url": make_absolute_url(img_src), "altText": img_alt}],
            "additionalImages": [],
            "aggregateRating": {
                "ratingValue": rating_val,
                "reviewCount": review_count,
                "bestRating": 5,
                "worstRating": 1,
            },
            "availability": "in_stock",
            "badges": badges,
            "promotions": promos,
            "variants": {
                "variantCount": len(v_options),
                "visibleOptions": v_options,
            },
            "description": "",
            "category": None,
            "features": [],
            "reviews": [],
            "seller": None,
            "serialNumbers": [],
            "specifications": [],
            "videos": [],
            "isSponsored": False,
            "priceRange": None,
            "availabilityMessage": None,
            "keyFeatures": []
        }
        products.append(p)

    return ScrapedData(products=products)

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic."""
    payload = {
        "api_key": API_KEY,
        "url": url,
        "optimize_request": True,
    }

    tries = 0
    success = False

    while tries <= retries and not success:
        try:
            proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)
            response = requests.get(proxy_url, timeout=30)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "lxml")
                scraped_data = extract_data(soup)
                
                if scraped_data and scraped_data.products:
                    pipeline.add_data(scraped_data)
                    success = True
                else:
                    logger.warning(f"No products extracted from {url}")
                    # Even if no products, we consider it "successful" extraction attempt
                    success = True 
            else:
                logger.warning(f"Request failed for {url} with status {response.status_code}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            try:
                future.result()
            except Exception as e:
                logger.error(f"Thread generated an exception: {e}")

if __name__ == "__main__":
    urls = [
        "https://www.decathlon.com/search?q=kids+shoes&options%5Bprefix%5D=last",
    ]

    logger.info("Starting concurrent scraping...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")