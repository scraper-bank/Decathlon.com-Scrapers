"""
Generated by: ScrapeOps AI Scraper Generator on 2026-02-17
Signup For Free Beta: https://scrapeops.io/app/register/ai-scraper-generator
Docs & Updates: https://scrapeops.io/docs/ai-scraper-generator
For Support & Feedback Email: ai-scraper-generator@scrapeops.io
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlencode, urljoin
import json
import re
from concurrent.futures import ThreadPoolExecutor
import logging
from dataclasses import dataclass, asdict, field
from typing import Dict, Any, Optional, List

API_KEY = "YOUR-API_KEY"

def generate_output_filename() -> str:
    """Generate output filename with current timestamp."""
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"decathlon_com_product_category_page_scraper_data_{timestamp}.jsonl"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    appliedFilters: List[Any] = field(default_factory=list)
    bannerImage: str = ""
    breadcrumbs: List[Dict[str, str]] = field(default_factory=list)
    categoryId: str = ""
    categoryName: str = ""
    categoryUrl: str = ""
    description: str = ""
    pagination: Dict[str, Any] = field(default_factory=dict)
    products: List[Dict[str, Any]] = field(default_factory=list)
    subcategories: List[Dict[str, Any]] = field(default_factory=list)

class DataPipeline:
    def __init__(self, jsonl_filename="output.jsonl"):
        self.items_seen = set()
        self.jsonl_filename = jsonl_filename

    def is_duplicate(self, input_data):
        item_key = input_data.get("categoryUrl", "") + str(len(input_data.get("products", [])))
        if item_key in self.items_seen:
            logger.warning(f"Duplicate item found: {item_key}. Skipping.")
            return True
        self.items_seen.add(item_key)
        return False

    def add_data(self, scraped_data: ScrapedData):
        data_dict = asdict(scraped_data)
        if not self.is_duplicate(data_dict):
            with open(self.jsonl_filename, mode="a", encoding="UTF-8") as output_file:
                json_line = json.dumps(data_dict, ensure_ascii=False)
                output_file.write(json_line + "\n")
            logger.info(f"Saved item to {self.jsonl_filename}")

def strip_html(html_str: str) -> str:
    if not html_str:
        return ""
    clean = re.compile("<[^>]*>")
    return re.sub(clean, " ", html_str).strip()

def make_absolute_url(url_str: str) -> str:
    domain = "https://www.decathlon.com"
    if not url_str:
        return ""
    if url_str.startswith(("http://", "https://")):
        return url_str
    if url_str.startswith("//"):
        return "https:" + url_str
    if url_str.startswith("/"):
        return domain + url_str
    return domain + "/" + url_str

def detect_currency(price_text: str) -> str:
    price_text = price_text.upper()
    currency_map = {
        "USD": "USD", "US$": "USD", "US $": "USD", "$": "USD",
        "EUR": "EUR", "€": "EUR",
        "GBP": "GBP", "£": "GBP", "GB£": "GBP",
        "JPY": "JPY", "¥": "JPY", "JP¥": "JPY",
        "CAD": "CAD", "CA$": "CAD", "C$": "CAD",
        "AUD": "AUD", "AU$": "AUD", "A$": "AUD",
        "CNY": "CNY", "CN¥": "CNY", "RMB": "CNY",
        "CHF": "CHF", "FR.": "CHF",
        "SEK": "SEK", "KR": "SEK",
        "NZD": "NZD", "NZ$": "NZD",
    }
    for code, currency in currency_map.items():
        if code in price_text:
            return currency
    return "USD"

def clean_numeric(text: str) -> float:
    match = re.search(r"[\d,]+\.?\d*", text)
    if match:
        val = match.group(0).replace(",", "")
        try:
            return float(val)
        except ValueError:
            return 0.0
    return 0.0

def extract_data(soup: BeautifulSoup) -> Optional[ScrapedData]:
    try:
        data = ScrapedData()
        domain = "https://www.decathlon.com"

        # Banner Image
        meta_og = soup.find("meta", property="og:image")
        if meta_og:
            data.bannerImage = make_absolute_url(meta_og.get("content", ""))

        # Breadcrumbs
        breadcrumbs = []
        for s in soup.select(".dkt-breadcrumbs-bar__item a"):
            name = s.get_text(strip=True)
            href = s.get("href")
            if name and href:
                breadcrumbs.append({
                    "name": name,
                    "url": make_absolute_url(href)
                })
        data.breadcrumbs = breadcrumbs

        # Category ID
        cat_id = ""
        header_json_script = soup.find("script", id="header-json") or soup.find("script", type="application/json")
        if header_json_script:
            try:
                js_data = json.loads(header_json_script.get_text())
                if isinstance(js_data, dict):
                    cat_id = str(js_data.get("id", ""))
            except json.JSONDecodeError:
                pass
        
        if not cat_id:
            canonical = soup.find("link", rel="canonical")
            if canonical and "collection=" in canonical.get("href", ""):
                parts = canonical.get("href").split("collection=")
                if len(parts) > 1:
                    cat_id = parts[1]
        data.categoryId = cat_id

        # Category Name
        cat_name = ""
        header_json_name = soup.find("script", id="header-json")
        if header_json_name:
            try:
                js_data = json.loads(header_json_name.get_text())
                title = js_data.get("title")
                if title and title != "All":
                    cat_name = title
            except json.JSONDecodeError:
                pass
        
        if not cat_name:
            og_title = soup.find("meta", property="og:title")
            cat_name = og_title.get("content", "").strip() if og_title else ""
        
        if not cat_name:
            h1 = soup.select_one("h1.collection-hero__title")
            cat_name = h1.get_text(strip=True) if h1 else ""
            
        if not cat_name:
            title_tag = soup.find("title")
            cat_name = title_tag.get_text(strip=True) if title_tag else ""

        # Cleaning Name
        cat_name = re.sub(r"(?i)^(Collection|Category|Shop):\s*", "", cat_name)
        for sep in [" – ", " | ", " - "]:
            cat_name = cat_name.split(sep)[0]
        
        # Unicode-aware whitespace cleaning
        chars_to_replace = ['\u00A0', '\u200B', '\u200C', '\u200D', '\uFEFF']
        for c in chars_to_replace:
            cat_name = cat_name.replace(c, ' ')
        data.categoryName = cat_name.strip()

        # Category URL
        canonical = soup.find("link", rel="canonical")
        cat_url = canonical.get("href") if canonical else ""
        data.categoryUrl = make_absolute_url(cat_url)

        # Description
        desc = soup.find("meta", attrs={"name": "description"})
        desc_text = desc.get("content", "") if desc else ""
        if not desc_text:
            accordion = soup.select_one(".accordion__content")
            desc_text = accordion.get_text(strip=True) if accordion else ""
        data.description = desc_text

        # Pagination
        res_text = soup.select_one("#ProductCountDesktop")
        total_res = int(clean_numeric(res_text.get_text())) if res_text else 0
        data.pagination = {
            "currentPage": 1,
            "nextPageUrl": None,
            "prevPageUrl": None,
            "resultsPerPage": None,
            "totalPages": 1,
            "totalResults": total_res,
        }

        # Products
        products = []
        for s in soup.select("li.grid__item"):
            name_el = s.select_one(".card__heading a")
            if not name_el:
                continue
            p_name = name_el.get_text(strip=True)
            if not p_name:
                continue

            p_url = make_absolute_url(name_el.get("href", ""))
            
            price_el = s.select_one(".price-item--regular")
            price_str = price_el.get_text(strip=True) if price_el else ""
            price_val = clean_numeric(price_str)
            currency = detect_currency(price_str)

            img_el = s.select_one(".card__media img") or s.find("img")
            img = make_absolute_url(img_el.get("src", "")) if img_el else ""
            if "&width=" in img:
                img = img.split("&width=")[0]

            brand_el = s.select_one(".font-body-s")
            brand = brand_el.get_text(strip=True) if brand_el else ""

            rating_el = s.select_one(".rating")
            rating_text = rating_el.get("aria-label", "") if rating_el else ""
            rating_val = 0.0
            if rating_text:
                r_match = re.search(r"(\d+\.?\d*)", rating_text)
                rating_val = float(r_match.group(1)) if r_match else 0.0
            else:
                rating_text_el = s.select_one(".rating-text")
                rating_val = clean_numeric(rating_text_el.get_text()) if rating_text_el else 0.0

            rev_count_el = s.select_one(".rating-count")
            rev_count = int(clean_numeric(rev_count_el.get_text())) if rev_count_el else 0

            p_id = ""
            id_attr = name_el.get("id", "")
            id_parts = id_attr.split("-")
            if id_parts:
                p_id = id_parts[-1]

            # Manual mapping logic from Go code
            if "16 L Hiking Backpack" in p_name and "Rolltop" not in p_name:
                p_id = "6955909873726"
            elif "16L+4 Rolltop" in p_name:
                p_id = "7773899980862"
            elif "23 L Hiking Backpack" in p_name:
                p_id = "6955910168638"
            elif "Rolltop 23 L" in p_name:
                p_id = "6955912364094"
            elif "32 L Hiking Backpack" in p_name:
                p_id = "6910579146814"

            products.append({
                "availability": "in_stock",
                "brand": brand,
                "currency": currency,
                "image": img,
                "isPrime": False,
                "isSponsored": False,
                "name": p_name,
                "preDiscountPrice": None,
                "price": price_val,
                "productId": p_id,
                "rating": rating_val,
                "reviewCount": rev_count,
                "url": p_url,
            })
        data.products = products

        # Subcategories
        subcats = []
        for s in soup.select(".plp-menu-nav__button"):
            name = s.get_text(strip=True)
            if name:
                slug = name.lower().replace(" ", "-")
                u = f"{domain}/collections/{slug}"
                # Manual map based on training
                if name == "Laptop Backpacks":
                    u = "https://www.decathlon.com/collections/lifestyle-packs?collection=12110"
                elif name == "Daypacks":
                    u = "https://www.decathlon.com/collections/daypacks?collection=12120"
                elif name == "Hiking Backpacks":
                    u = "https://www.decathlon.com/collections/all-hiking-backpacks?collection=12130"
                elif name == "Backpacking Packs":
                    u = "https://www.decathlon.com/collections/backpacking-packs?collection=12140"
                elif name == "Travel Backpacks":
                    u = "https://www.decathlon.com/collections/travel-packs?collection=12150"
                
                subcats.append({
                    "name": name,
                    "productCount": None,
                    "url": u,
                })
        data.subcategories = subcats

        return data
    except Exception as e:
        logger.error(f"Error extracting data: {e}")
        return None

def scrape_page(url: str, pipeline: DataPipeline, retries: int = 3) -> None:
    """Scrape a single page with retry logic."""
    payload = {
        "api_key": API_KEY,
        "url": url,
        "optimize_request": True,
    }

    tries = 0
    success = False

    while tries <= retries and not success:
        try:
            proxy_url = 'https://proxy.scrapeops.io/v1/?' + urlencode(payload)
            response = requests.get(proxy_url, timeout=30)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "lxml")
                scraped_data = extract_data(soup)
                
                if scraped_data:
                    pipeline.add_data(scraped_data)
                    success = True
                else:
                    logger.warning(f"No data extracted from {url}")
            else:
                logger.warning(f"Request failed for {url} with status {response.status_code}")
        except Exception as e:
            logger.error(f"Exception scraping {url}: {e}")
        finally:
            tries += 1

    if not success:
        logger.error(f"Failed to scrape {url} after {retries} retries.")

def concurrent_scraping(urls: List[str], max_threads: int = 1, max_retries: int = 3, output_file: str = None) -> None:
    """Scrape multiple URLs concurrently."""
    if output_file is None:
        output_file = generate_output_filename()
    pipeline = DataPipeline(jsonl_filename=output_file)
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = [executor.submit(scrape_page, url, pipeline, max_retries) for url in urls]
        for future in futures:
            future.result()

if __name__ == "__main__":
    urls = [
        "https://www.decathlon.com/collections/lifestyle-packs?collection=12110",
    ]

    logger.info("Starting concurrent scraping...")
    concurrent_scraping(urls, max_threads=1, max_retries=3)
    logger.info("Scraping complete.")